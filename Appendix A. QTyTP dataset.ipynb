{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A: Multilingual  Dataset of Type Annotated Question Pairs QTyTP\n",
    "\n",
    "\n",
    "This appendix reports on the collection and processing of a multilingual question pair dataset developed for research on cross-lingual semantic similarity and the representation of interrogative intent in pretrained language models. The dataset contains parallel question pairs in five languages: Afrikaans, Arabic, English, Indonesian, and Marathi. Questions were extracted from the NLLB corpus (Schwenk 2020, Tiedemann 2012) using language-specific patterns and aligned based on their translations. English sentences were annotated with linguistic features capturing information type (e.g., modality, quantification, cleft) and question type (e.g., polar, wh-questions, alternative, conditional) using rule-based approaches. Preprocessing steps, including filtering, deduplication, and language balancing, were applied to ensure data quality and composition.\n",
    "\n",
    "The result is a dataset containing ~100k question pairs adapted for investigating the relationships between language-specific features, semantic similarity, and the encoding of questions in multilingual models. The collection and processing pipeline described here is customisable to some extent and allows for varying sizes and dataset properties. However, it is important to note that this script requires local acces to NLLB alignment files and monolingual corpora, which are quite large and sometimes difficult to handle efficiently. Overall, this resource aims to support the development and evaluation of cross-lingual approaches to sentence processing tasks and to focuson the linguistic factors shaping the representation of interrogative semantics.\n",
    "\n",
    "In the final step, the annotated dataset is encoded using a SciKit library called Multi Label Binarizer. Since our two kinds of features are stored in the dataset as an iterable, we use the MLB transformer to turn it into a single multilabel format. This kind Multi Label Encoding is popular for use in annotating data samples with multiple features of different kind. For more info, consult the documentation (https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html#sklearn.preprocessing.MultiLabelBinarizer).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QuestionDetector class uses regular expressions to detect questions across five languages: English, Marathi, Arabic, Indonesian, and Afrikaans. The class initializes with language-specific pattern dictionaries that contain regular expressions for identifying three main types of question indicators: WH-question words (like \"who,\" \"what,\" \"where\" in English and their translations in other languages), auxiliary verb patterns for yes/no questions, and question marks (including the Arabic question mark '؟', and Marathi particles का, काय)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing AF...\n",
      "Saved 78069 question-translation pairs to question_pairs_af.json\n",
      "Processing AR...\n",
      "Saved 41697 question-translation pairs to question_pairs_ar.json\n",
      "Processing ID...\n",
      "Saved 104867 question-translation pairs to question_pairs_id.json\n",
      "Processing MR...\n",
      "Saved 189036 question-translation pairs to question_pairs_mr.json\n",
      "\n",
      "Saved all question-translation pairs to all_question_pairs.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This script defines the patterns for data collection from NLLB source, target, and alignment file.\n",
    "It essentially searches both the source language and target language monolingual corpora files by line index, and appends them to a new json object holding original index for cross reference and the pair of sentence.\n",
    "\n",
    "The patterns were collected with the help of Claude 3.5 Sonnet, by giving it prompts to recognize the question words in a select sample of sentences from the data.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class QuestionDetector:\n",
    "    def __init__(self):\n",
    "\n",
    "        # English question patterns\n",
    "        self.en_patterns = [\n",
    "            r'^(Who|What|Where|When|Why|How|Which|Whose|Whom)\\b',  # WH question key words\n",
    "            r'^(Do|Does|Did|Is|Are|Was|Were|Have|Has|Had|Can|Could|Should|Would|Will)\\b',  # polar questions\n",
    "            r'\\?$'  # Question mark at end\n",
    "        ]\n",
    "        \n",
    "        # Marathi question patterns\n",
    "        self.mr_patterns = [\n",
    "            r'^(कोण|काय|कुठे|केव्हा|का|कसे|कोणता|कोणाचे|कोणाला)\\b',  # WH questions\n",
    "            r'\\?$',  # Question mark\n",
    "            r'(का|काय)$'  # Question particles at end\n",
    "        ]\n",
    "        \n",
    "        # Arabic question patterns\n",
    "        self.ar_patterns = [\n",
    "            r'^(من|ما|ماذا|أين|متى|لماذا|كيف|أي|لمن|هل)\\b',  # WH questions and هل\n",
    "            r'\\?$',  # Question mark\n",
    "            r'؟$'    # Arabic question mark\n",
    "        ]\n",
    "        \n",
    "        # Indonesian question patterns\n",
    "        self.id_patterns = [\n",
    "            r'^(Siapa|Apa|Dimana|Kapan|Mengapa|Bagaimana|Yang mana|Kepada siapa)\\b',  # WH questions\n",
    "            r'^Apakah\\b',  # Yes/No questions\n",
    "            r'\\?$'  # Question mark\n",
    "        ]\n",
    "        \n",
    "        # Afrikaans question patterns\n",
    "        self.af_patterns = [\n",
    "            r'^(Wie|Wat|Waar|Wanneer|Hoekom|Hoe|Watter|Aan wie)\\b',  # WH questions\n",
    "            r'^(Is|Het|Sal|Kan|Moet|Wil|Mag)\\b',  # Verb-initial questions\n",
    "            r'\\?$'  # Question mark\n",
    "        ]\n",
    "        \n",
    "        # Map language codes to their patterns\n",
    "        self.lang_patterns = {\n",
    "            'en': self.en_patterns,\n",
    "            'mr': self.mr_patterns,\n",
    "            'ar': self.ar_patterns,\n",
    "            'id': self.id_patterns,\n",
    "            'af': self.af_patterns\n",
    "        }\n",
    "\n",
    "\n",
    "    def is_question(self, text, language):\n",
    "        \n",
    "        \"\"\"Check if a sentence from the source file is a question in the specified language.\"\"\"\n",
    "\n",
    "        if not text or language not in self.lang_patterns: # data validation\n",
    "            return False\n",
    "            \n",
    "        patterns = self.lang_patterns[language] # loads the regex patterns\n",
    "        \n",
    "        # module that searches for the sentence data\n",
    "        for pattern in patterns:\n",
    "            if re.search(pattern, text):\n",
    "\n",
    "                # Additional validation: must end with question mark for most cases\n",
    "\n",
    "                if language != 'mr':  #  marathi sometimes uses a particle mark at the end\n",
    "                    return text.rstrip().endswith('?') or text.rstrip().endswith('؟')\n",
    "                \n",
    "                return True                \n",
    "        return False\n",
    "\n",
    "\n",
    "def extract_question_pairs(source_file: str, target_file: str, language: str, limit: int = 2000000):\n",
    "\n",
    "    \"\"\"\n",
    "    Main function that extracts question-translation pairs from source and target language files. The limit can be changed for controlling the size of the dataset, i.e. number of lines in the monolingual files to search through.\n",
    "\n",
    "    Takes: source_file: Path to the source language file.\n",
    "        target_file: Path to the target language file (English).\n",
    "        language: The language code of the source file.\n",
    "        limit: The maximum number of lines to read from each file.\n",
    "\n",
    "    Returns:\n",
    "        indices of questions in the source file, dictionaries containing the source-target translation pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    detector = QuestionDetector()\n",
    "    question_pairs = {} # empty dict for storing pairs and indices\n",
    "\n",
    "    try:\n",
    "        with open(source_file, 'r', encoding='utf-8') as sf, open(target_file, 'r', encoding='utf-8') as tf: # opens the file and reads lines\n",
    "            source_lines = [line.strip() for line in sf.readlines()[:limit]]\n",
    "            target_lines = [line.strip() for line in tf.readlines()[:limit]]  # reads the entire line\n",
    "\n",
    "            for i, source_text in enumerate(source_lines): # for every index. check if it holds a question\n",
    "                if detector.is_question(source_text, language):\n",
    "                    question_pairs[i] = {\"source\": source_text, \"target\": target_lines[i]} # Append translation and source\n",
    "\n",
    "\n",
    "\n",
    "    except FileNotFoundError: # Error handling for misplaced files\n",
    "        print(f\"Error: {source_file} or {target_file} not found.\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return {}\n",
    "\n",
    "    return question_pairs\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process language files and extract question pairs. If youre working with other language pairs, change the file structure in language_pairs dict\n",
    "    \"\"\" \n",
    "    \n",
    "    language_pairs = {\n",
    "        'af': 'NLLB.af-en',\n",
    "        'ar': 'NLLB.ar-en',\n",
    "        'id': 'NLLB.en-id',\n",
    "        'mr': 'NLLB.en-mr'\n",
    "    }\n",
    "\n",
    "    all_question_pairs = {} # empty dict to compile all language specific results\n",
    "\n",
    "\n",
    "    for lang, file_base in language_pairs.items(): # loop through your language files, make sure the path is correct for your corpus files\n",
    "        source_file = f\"{file_base}.{lang}\"\n",
    "        target_file = f\"{file_base}.en\"  # English target\n",
    "        output_file = f\"question_pairs_{lang}.json\"\n",
    "\n",
    "        print(f\"Processing {lang.upper()}...\")\n",
    "        try:\n",
    "            question_pairs = extract_question_pairs(source_file, target_file, lang) # call the main collection function, takes three inputs, saves output to question_pairs\n",
    "            all_question_pairs[lang] = question_pairs\n",
    "\n",
    "            with open(output_file, 'w', encoding='utf-8') as outfile: # creates new output file  \n",
    "                json.dump(question_pairs, outfile, ensure_ascii=False, indent=2)\n",
    "\n",
    "            print(f\"Saved {len(question_pairs)} question-translation pairs to {output_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {lang}: {str(e)}\")\n",
    "\n",
    "\n",
    "    combined_output_file = \"all_question_pairs.json\"\n",
    "    with open(combined_output_file, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(all_question_pairs, outfile, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"\\nSaved all question-translation pairs to {combined_output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data framing\n",
    "\n",
    "This module creates a dataframe from the json dictionary of questions. The 'create_dataframe_from_json' function reads multiple JSON files containing question pairs in different languages (Afrikaans, Arabic, Indonesian, and Marathi) and converts them into a pandas DataFrame. For each JSON file, it extracts the source question, target translation, and adds two empty data columns as placeholder lists for two types of features and adds a language tag for each source sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  index                                             source  \\\n",
      "0    37  Hoekom het jy nie meer tyd om my te sien nie?'...   \n",
      "1    54  Wat is die grootste openbaring van geloof en h...   \n",
      "2    75  In 2011 die wêreld-ekonomie sal groei ten kost...   \n",
      "3    89        \"Wat as dit iets ernstigs is, selfs kanker?   \n",
      "4    92  Die National Youth Leadership Forum: moet u gaan?   \n",
      "\n",
      "                                              target language feature1  \\\n",
      "0  Why don't you have more time to see me?\" and \"...       af       []   \n",
      "1  What is the greatest revelation of faith, and ...       af       []   \n",
      "2  In 2011 the world economy will grow at the exp...       af       []   \n",
      "3      \"What if it's something serious; even cancer?       af       []   \n",
      "4  The National Youth Leadership Forum: Should Yo...       af       []   \n",
      "\n",
      "  feature2  \n",
      "0       []  \n",
      "1       []  \n",
      "2       []  \n",
      "3       []  \n",
      "4       []  \n",
      "DataFrame saved to nllb_all_questionpairs.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def create_dataframe_from_json(json_filepaths, languages):\n",
    "    \"\"\"\n",
    "    Creates a Pandas DataFrame from pairs of JSON files.\n",
    "    Adds empty columns for language, feature1, and feature2 (initialized as empty lists).\n",
    "\n",
    "    takes:\n",
    "        A list of filepaths to the JSON files.\n",
    "        A list of language codes corresponding to the JSON files.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    data = [] # this is our empty data structure, list \n",
    "\n",
    "    for filepath, lang in zip(json_filepaths, languages): # iterates over the entire language file and appends pairs to new data list\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                question_pairs = json.load(f)\n",
    "                for idx, pair in question_pairs.items():\n",
    "                    # part below shows the structure of our dataset \n",
    "                    data.append({\n",
    "                        'index': idx,\n",
    "                        'source': pair.get('source', ''),\n",
    "                        'target': pair.get('target', ''),\n",
    "                        'language': lang, # tag from ['af', 'ar', 'id', 'mr']\n",
    "                        'feature1': [],  # feature1 empty list\n",
    "                        'feature2': []  \n",
    "                    })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    filepaths = [\n",
    "        \"question_pairs_af.json\",  # make sure paths are correct\n",
    "        \"question_pairs_ar.json\",\n",
    "        \"question_pairs_id.json\",\n",
    "        \"question_pairs_mr.json\",\n",
    "    ]\n",
    "    languages = ['af', 'ar', 'id', 'mr']\n",
    "\n",
    "\n",
    "\n",
    "    df = create_dataframe_from_json(filepaths, languages)\n",
    "\n",
    "        \n",
    "    if df is not None:\n",
    "\n",
    "        print(df.head())  # Print first few rows for inspection\n",
    "\n",
    "        \n",
    "        df.to_csv(\"nllb_all_questionpairs.csv\", index=False, encoding='utf-8') # save to csv format for easy loading into llm\n",
    "        print(\"DataFrame saved to nllb_all_questionpairs.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation rules\n",
    "This section focuses on annotating a DataFrame of text data with linguistic features. It defines a function annotate_features that searches the sentences for patterns of two kinds: feature1 identifies the \"Information Type\" present in the text (modality, quantification, comparison, negation, cleft sentences), while feature2 classifies the \"Question Type\" (polar, wh-question, alternative, conditional). Using regular expressions and stemming, the code analyzes the text to extract these features. The Porter Stemmer transforms sentences into lists of stems, and the Regex search pattern matches the defined structure to sentences in the dataset.\n",
    "\n",
    "Examples of annotated questions:\n",
    "\n",
    "1. Isn't the majority of them broken? [quantification] [polar]\n",
    "2. Would you rather have pizza or sushi? [modality] [alternative]\n",
    "3. Which one is more expensive? [comparison] [wh-question]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index                                             source  \\\n",
      "0     37  Hoekom het jy nie meer tyd om my te sien nie?'...   \n",
      "1     54  Wat is die grootste openbaring van geloof en h...   \n",
      "2     75  In 2011 die wêreld-ekonomie sal groei ten kost...   \n",
      "3     89        \"Wat as dit iets ernstigs is, selfs kanker?   \n",
      "4     92  Die National Youth Leadership Forum: moet u gaan?   \n",
      "\n",
      "                                              target language  \\\n",
      "0  Why don't you have more time to see me?\" and \"...       af   \n",
      "1  What is the greatest revelation of faith, and ...       af   \n",
      "2  In 2011 the world economy will grow at the exp...       af   \n",
      "3      \"What if it's something serious; even cancer?       af   \n",
      "4  The National Youth Leadership Forum: Should Yo...       af   \n",
      "\n",
      "                                           feature1       feature2  \n",
      "0  [modality, quantification, comparison, negation]  [wh-question]  \n",
      "1                                                []  [wh-question]  \n",
      "2                                        [modality]             []  \n",
      "3                                                []  [wh-question]  \n",
      "4                                        [modality]             []  \n",
      "Annotated data saved to nllb_annotated_questionpairs.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "def annotate_features(df):\n",
    "\n",
    "    \"\"\"Annotates the DataFrame with information type and question type features.\"\"\"\n",
    "\n",
    "    # Information Type (feature1)\n",
    "    \"\"\" searches for words that are indicative of the kind of propositions that are the focus of the sentence\"\"\"\n",
    "\n",
    "    def get_information_type(text):\n",
    "\n",
    "        text = text.lower()  # Lowercase for case-insensitivity\n",
    "        features = []\n",
    "\n",
    "        # Modality\n",
    "        if re.search(r\"\\b(can|could|should|would|will|may|might|must)\\b\", text):\n",
    "            features.append(\"modality\")\n",
    "\n",
    "        # Quantification\n",
    "        if re.search(r\"\\b(how much|how many|some|all|any|few|many|several|most|none)\\b\", text):\n",
    "            features.append(\"quantification\")\n",
    "\n",
    "        # Comparison\n",
    "        if re.search(r\"\\b(more|less|better|worse|bigger|smaller|than|as|equal|similar|different)\\b\", text):\n",
    "            features.append(\"comparison\")\n",
    "\n",
    "        #cleft sentences\n",
    "        cleft_pattern_wh = r\"it'?s?\\b.*\\b(that|who|which|where|when|why|how)\\b\"  # Note the '?' after 'it' and 's'\n",
    "        if re.search(cleft_pattern_wh, text, re.IGNORECASE):\n",
    "            features.append(\"cleft\")\n",
    "\n",
    "        # Negation\n",
    "        if re.search(r\"(\\bnot|n't|\\bno|\\bnever|\\bnobody|\\bnothing|\\bnowhere|\\bneither|\\bnor)\\b\", text):\n",
    "            features.append(\"negation\")\n",
    "\n",
    "        return features\n",
    "\n",
    "    # Question Type (feature2)\n",
    "\n",
    "    def get_question_type(text):\n",
    "        text = text.lower()  # Lowercase\n",
    "        stemmer = PorterStemmer()\n",
    "\n",
    "        text_stemmed = \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "        \n",
    "        if re.search(r\"^(?:\\bdo|doe|\\bdid|\\bis|are|wa|do|\\bdoes|did|is|\\bhas|were|have|ha|had|can|could|should|would|will|mai|might|must)\\b\", text_stemmed):\n",
    "            return [\"polar\"]\n",
    "        elif re.search(r\"(\\bwho|who|\\bwhat|\\bwhich|what|where|when|why|how|which|whose|whom)\\b\", text):\n",
    "            return [\"wh-question\"]\n",
    "        elif re.search(r\"\\bor\\b\", text):\n",
    "            return [\"alternative\"]\n",
    "        elif re.search(r\"\\bif\\b\", text):\n",
    "            return [\"conditional\"]\n",
    "        return []\n",
    "    \n",
    "\n",
    "    df['feature1'] = df['target'].apply(get_information_type) # update the empty columns with the results of running type checkers\n",
    "    df['feature2'] = df['target'].apply(get_question_type)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def main(): # main fucntion loads the csv file, annotates it and saves to output\n",
    "\n",
    "    input_csv = \"nllb_all_questionpairs.csv\" # check the output of the framing cell\n",
    "    output_csv = \"nllb_annotated_questionpairs.csv\" # note down the filepath for filtering step later\n",
    "\n",
    "    try: # check correct loading from CSV\n",
    "        df = pd.read_csv(input_csv, converters={'feature1': eval, 'feature2': eval})  \n",
    "        df = annotate_features(df)\n",
    "        print(df.head()) # Print the first few examples\n",
    "        df.to_csv(output_csv, index=False, encoding='utf-8')\n",
    "        print(f\"Annotated data saved to {output_csv}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {input_csv} not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final filter\n",
    "Cell bellow reads the file 'nllb_annotated_questionpairs.csv', processes the feature1 and feature2 columns to ensure they contain lists, filters out rows with empty lists in these columns, and saves the filtered data to a new file .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered data saved to qtytp-all.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def filter_feature_annotations(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Filters a CSV file of question pairs, keeping only rows where both features are present.\n",
    "    Removes rows with empty annotations and prints statistics about the filtering process.\n",
    "\n",
    "    takes two files:\n",
    "        input_file: Path to the input CSV file\n",
    "        output_file: Path to the output CSV file\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(input_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{input_file}' not found.\")\n",
    "        return\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"Error: Input CSV file is empty.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the CSV file: {e}\")\n",
    "        return\n",
    "\n",
    "    # this defines the content of the feature columns as necessarily strings, the argument of the apply function is described in this stack overal post[https://stackoverflow.com/questions/44061607/pandas-lambda-function-with-nan-support], i.e. it is pandas specific standardization of the df\n",
    "    \n",
    "    df['feature1'] = df['feature1'].astype(str).apply(lambda x: eval(x) if x != 'nan' and x != '[]' else [])\n",
    "    df['feature2'] = df['feature2'].astype(str).apply(lambda x: eval(x) if x != 'nan' and x != '[]' else [])\n",
    "\n",
    "    # filter for rows with non-empty features, this is an interesting control factor to play around with\n",
    "    df_filtered = df[\n",
    "        (df['feature1'].apply(len) > 0) & \n",
    "        (df['feature2'].apply(len) > 0)\n",
    "    ].copy()\n",
    "\n",
    "   \n",
    "    try:\n",
    "        df_filtered.to_csv(output_file, index=False)\n",
    "        print(f\"\\nFiltered data saved to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to output file: {e}\")\n",
    "\n",
    "# change file paths to your needs\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv = 'nllb_annotated_questionpairs.csv'\n",
    "    output_csv = 'qtytp-all.csv'\n",
    "    filter_feature_annotations(input_csv, output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Dataset statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'af': {'pairs_count': 23881, 'fully_annotated': 23881}, 'ar': {'pairs_count': 19699, 'fully_annotated': 19699}, 'id': {'pairs_count': 50584, 'fully_annotated': 50584}, 'mr': {'pairs_count': 41352, 'fully_annotated': 41352}, 'overall': {'pairs_count': 135516, 'fully_annotated': 135516}, 'feature_counts': {'[': {'total': 271032, 'per_language': {'af': 47762, 'ar': 39398, 'id': 101168, 'mr': 82704}}, \"'\": {'total': 626184, 'per_language': {'af': 110970, 'ar': 91308, 'id': 241082, 'mr': 182824}}, 'm': {'total': 97832, 'per_language': {'af': 16261, 'ar': 16896, 'id': 40851, 'mr': 23824}}, 'o': {'total': 325900, 'per_language': {'af': 57837, 'ar': 48558, 'id': 126036, 'mr': 93469}}, 'd': {'total': 78634, 'per_language': {'af': 12287, 'ar': 13109, 'id': 32474, 'mr': 20764}}, 'a': {'total': 247501, 'per_language': {'af': 43382, 'ar': 36922, 'id': 96441, 'mr': 70756}}, 'l': {'total': 127644, 'per_language': {'af': 20955, 'ar': 20911, 'id': 51715, 'mr': 34063}}, 'i': {'total': 336970, 'per_language': {'af': 59383, 'ar': 48213, 'id': 130572, 'mr': 98802}}, 't': {'total': 292489, 'per_language': {'af': 51463, 'ar': 40705, 'id': 112520, 'mr': 87801}}, 'y': {'total': 74200, 'per_language': {'af': 11703, 'ar': 12587, 'id': 30470, 'mr': 19440}}, ',': {'total': 42060, 'per_language': {'af': 7723, 'ar': 6256, 'id': 19373, 'mr': 8708}}, ' ': {'total': 42060, 'per_language': {'af': 7723, 'ar': 6256, 'id': 19373, 'mr': 8708}}, 'q': {'total': 123895, 'per_language': {'af': 22141, 'ar': 17275, 'id': 46632, 'mr': 37847}}, 'u': {'total': 123895, 'per_language': {'af': 22141, 'ar': 17275, 'id': 46632, 'mr': 37847}}, 'n': {'total': 269018, 'per_language': {'af': 49943, 'ar': 34181, 'id': 99358, 'mr': 85536}}, 'f': {'total': 41823, 'per_language': {'af': 7512, 'ar': 5900, 'id': 17293, 'mr': 11118}}, 'c': {'total': 69889, 'per_language': {'af': 12654, 'ar': 10731, 'id': 29678, 'mr': 16826}}, 'p': {'total': 57384, 'per_language': {'af': 10436, 'ar': 10184, 'id': 22732, 'mr': 14032}}, 'r': {'total': 62492, 'per_language': {'af': 11282, 'ar': 10815, 'id': 24886, 'mr': 15509}}, 's': {'total': 115854, 'per_language': {'af': 21131, 'ar': 16980, 'id': 44456, 'mr': 33287}}, 'e': {'total': 150509, 'per_language': {'af': 28040, 'ar': 18388, 'id': 54932, 'mr': 49149}}, 'g': {'total': 37921, 'per_language': {'af': 7831, 'ar': 3159, 'id': 11813, 'mr': 15118}}, ']': {'total': 271032, 'per_language': {'af': 47762, 'ar': 39398, 'id': 101168, 'mr': 82704}}, 'w': {'total': 92222, 'per_language': {'af': 16573, 'ar': 12671, 'id': 34075, 'mr': 28903}}, 'h': {'total': 92222, 'per_language': {'af': 16573, 'ar': 12671, 'id': 34075, 'mr': 28903}}, '-': {'total': 92222, 'per_language': {'af': 16573, 'ar': 12671, 'id': 34075, 'mr': 28903}}, 'v': {'total': 5108, 'per_language': {'af': 846, 'ar': 631, 'id': 2154, 'mr': 1477}}}}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def analyze_question_pairs(csv_file):\n",
    "    \"\"\"\n",
    "    Analyzes question pairs from a CSV file.\n",
    "\n",
    "    Takes csv_file: path to the CSV file.\n",
    "\n",
    "    Returns a dictionary containing the analysis results.  Returns None if there's an error.\n",
    "    \"\"\"\n",
    "    \n",
    "    try: # reads the filtered questions file, takes output of previous cell block\n",
    "        df = pd.read_csv(csv_file)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the CSV file: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    results = {}\n",
    "    languages = df['language'].unique() # list of languages [id_1, id_2, id_3, ..]\n",
    "\n",
    "    for lang in languages:\n",
    "        lang_df = df[df['language'] == lang]\n",
    "        results[lang] = {\n",
    "            'pairs_count': len(lang_df),\n",
    "            'fully_annotated': len(lang_df[(lang_df['feature1'].apply(len) > 0) & (lang_df['feature2'].apply(len) > 0)])}\n",
    "        \n",
    "    overall = {\n",
    "        'pairs_count': len(df),\n",
    "        'fully_annotated': len(df[(df['feature1'].apply(len) > 0) & (df['feature2'].apply(len) > 0)])}\n",
    "    \n",
    "    feature_counts = {}\n",
    "    for col in ['feature1', 'feature2']:\n",
    "        for _, row in df.iterrows():\n",
    "            for feature in row[col]:\n",
    "                if feature not in feature_counts:\n",
    "                    feature_counts[feature] = {'total': 0, 'per_language': {}}\n",
    "                if col == 'feature1':\n",
    "                    feature_counts[feature]['per_language'][row['language']] = feature_counts[feature].get('per_language', {}).get(row['language'], 0) + 1\n",
    "                    feature_counts[feature]['total'] += 1\n",
    "                elif col == 'feature2':\n",
    "                    feature_counts[feature]['per_language'][row['language']] = feature_counts[feature].get('per_language', {}).get(row['language'], 0) + 1\n",
    "                    feature_counts[feature]['total'] += 1\n",
    "    \n",
    "    results['overall'] = overall\n",
    "    results['feature_counts'] = feature_counts\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "analysis_results = analyze_question_pairs('qtytp-all.csv')\n",
    "if analysis_results:\n",
    "    print(analysis_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Label Binary Feature Encoding\n",
    "\n",
    "Takes feature columns that contain strings of feature names. MLE transforms these lists into a binary matrix. Each column represents a binary value for the presence of a label. More information about sklearn-MLB and the fit_transform and prepare_multilabel_encoding functions see documentation here [https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html#sklearn.preprocessing.MultiLabelBinarizer.fit_transform]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded data saved to: qtytp-all-encoded.csv\n",
      "   index                                             source  \\\n",
      "0     37  Hoekom het jy nie meer tyd om my te sien nie?'...   \n",
      "1    141  En met wie het die Mond van JaHWeH gespreek, d...   \n",
      "2    168  Weet u ooit hoekom ons nie 'n Duitsland kan we...   \n",
      "3    243  Dink jy dit sou so suksesvol gewees het as Piz...   \n",
      "4    281  Hoekom kan jy nie net oor hulle loop nie, weet...   \n",
      "\n",
      "                                              target language  \\\n",
      "0  Why don't you have more time to see me?\" and \"...       af   \n",
      "1  Who is he to whom the mouth of Yahweh has spok...       af   \n",
      "2        Do you ever know why we can't be a Germany?       af   \n",
      "3  Do you think it would have been as successful ...       af   \n",
      "4  Why can't you just walk over them, you know, l...       af   \n",
      "\n",
      "                                           feature1       feature2  f1_cleft  \\\n",
      "0  [modality, quantification, comparison, negation]  [wh-question]         0   \n",
      "1                                        [modality]  [wh-question]         0   \n",
      "2                              [modality, negation]        [polar]         0   \n",
      "3                            [modality, comparison]        [polar]         0   \n",
      "4              [modality, quantification, negation]  [wh-question]         0   \n",
      "\n",
      "   f1_comparison  f1_modality  f1_negation  f1_quantification  f2_alternative  \\\n",
      "0              1            1            1                  1               0   \n",
      "1              0            1            0                  0               0   \n",
      "2              0            1            1                  0               0   \n",
      "3              1            1            0                  0               0   \n",
      "4              0            1            1                  1               0   \n",
      "\n",
      "   f2_conditional  f2_polar  f2_wh-question  \n",
      "0               0         0               1  \n",
      "1               0         0               1  \n",
      "2               0         1               0  \n",
      "3               0         1               0  \n",
      "4               0         0               1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "def prepare_multilabel_encoding(df):\n",
    "    \"\"\"\n",
    "    prepares multi-label encodings for 'feature1' and 'feature2' columns in our dataset\n",
    "\n",
    "    Takes the final filtered df with 'feature1' and 'feature2' columns containing lists of strings\n",
    "\n",
    "    Returns pandas df with additional columns for multi-label encoding <- binary matrix\n",
    "    \"\"\"\n",
    "    # check if the features are in the correct list format, convert string representations of lists to actual lists if needed\n",
    "    \n",
    "    df['feature1'] = df['feature1'].apply(eval) if isinstance(df['feature1'].iloc[0], str) else df['feature1']\n",
    "    df['feature2'] = df['feature2'].apply(eval) if isinstance(df['feature2'].iloc[0], str) else df['feature2']\n",
    "\n",
    "\n",
    "    # Feature 1 MLB encoding\n",
    "    mlb_feature1 = MultiLabelBinarizer() # create an instance of the MLB class\n",
    "    encoded_feature1 = mlb_feature1.fit_transform(df['feature1']) # fit transform\n",
    "    encoded_feature1_df = pd.DataFrame(encoded_feature1, columns=[f\"f1_{label}\" for label in mlb_feature1.classes_])\n",
    "    df = pd.concat([df.reset_index(drop=True), encoded_feature1_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # repeat steps above\n",
    "    mlb_feature2 = MultiLabelBinarizer()\n",
    "    encoded_feature2 = mlb_feature2.fit_transform(df['feature2'])\n",
    "    encoded_feature2_df = pd.DataFrame(encoded_feature2, columns=[f\"f2_{label}\" for label in mlb_feature2.classes_])\n",
    "    df = pd.concat([df.reset_index(drop=True), encoded_feature2_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "   \n",
    "    data = pd.read_csv(\"qtytp-all.csv\")\n",
    "\n",
    "    # create a multi-label encoding function instance \n",
    "    encoded_data = prepare_multilabel_encoding(data.copy()) \n",
    "\n",
    "    # save the encoded data to a new CSV file\n",
    "    output_csv_path = \"qtytp-all-encoded.csv\" # change this if you want to save it somewhere else\n",
    "    encoded_data.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
    "    print(f\"Encoded data saved to: {output_csv_path}\")\n",
    "\n",
    "\n",
    "    print(encoded_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Schwenk, Holger, Guillaume Wenzek, Sergey Edunov, Edouard Grave, and Armand Joulin. \"CCMatrix: Mining Billions of High-Quality Parallel Sentences on the WEB.\" arXiv preprint (2020) arXiv:1911.04944 \n",
    "\n",
    "Tiedemann, Jörg. \"Parallel Data, Tools and Interfaces in OPUS.\" In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12), 2214–18. Istanbul, Turkey: European Language Resources Association (ELRA).  (2012) https://opus.nlpl.eu/NLLB/corpus/version/NLLB"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
