{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9939346b",
   "metadata": {},
   "source": [
    "# Mapping Interrogative Meanings: Cross-lingual Experiments With Transformer Representations of Questions\n",
    "\n",
    "##### Abstract\n",
    "\n",
    "This research explores the semantic representation of questions across multiple languages using the 'paraphrase-multilingual-MiniLM-L12-v2' sentence transformer.  We investigate how this model captures the nuances of interrogative intent and propositional content. By computing the cosine similarity of embedding pairs and analyzing their co-occurrence with different linguistic properties, we aim to identify cross-lingual and language-specific patterns in the way that this model encodes questions. This investigation can improve our understanding of interrogative intent in NLP and contribute to better cross-lingual applications. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e4a828",
   "metadata": {
    "id": "SOerPV3ckmRE"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In traditional approaches to sentence similarity, the goal is usually to determine how similar two sentences are in terms of their meaning or semantic content. This particular group of tasks, often called Semantic Textual Similarity (STS), is used in Natural Language Processing (NLP) as a way to either evaluate the performance of Language Models (LLMs) or to measure semantic overlap between text documents. There are different kinds of STS tasks, and a natural way to group them is by the specific type of similarity they focus on. Consider the following pairs of sentences that can be described as similar (in both cases, A is similar to B), but notice that each measure can be described differently:\n",
    "\n",
    "1. A: This movie is worth watching. B: Watching this film will change your life!\n",
    "2. A: John never married. B: John remained a bachelor.\n",
    "\n",
    "We could say that the first two sentences are similar because they have a similar (positive) sentiment. However, the second pair of sentences is not similar in the same sense. This simple example illustrates that there are different kinds and degrees of semantic similarity. The kind of semantic similarity discussed here best illustrated with the following example:\n",
    "\n",
    "3. A: Is this movie better or worse than the last one you saw? B: Isn't it this movie that is better than the last one?\n",
    "\n",
    "One way of describing the similarity between these two questions is by pointing out that the propositional content of the two sentences is about similar things, and both sentences use the content in a similar function, namely to compare. To properly account for the way in which these sentences are similar we would have to map the content of the question in the semantic space of all sentences and approximate similarity with distance. In the case of sentences from different languages, performance usually correlates with the capacity to represent translations as having similar semantic content. Luckily for us, high-performing models allow us to investigate the ways in which the questions from the example above are distinct. Therefore, instead of focusing on general sentence similarity, this report focuses specifically on interrogative sentences, which share the common function of raising an issue or eliciting information. This allows us to probe vector representations of natural language sentences that share similar linguistic properties, in this case the context of interrogative intent. Interrogative intent is a pragmatic phenomenon which makes us understand the semantic content of questions as raising issues or eliciting information, and the goal of this study is to explore the relation between this intent, propositional content and internal vector representations in multilingual LLMs.\n",
    "\n",
    "Modern NLP techniques rely heavily on text embeddings. These are dense vector representations of text that capture semantic and syntactic information. While there has been extensive research on *sentence* embeddings, understanding features of any sentence as a whole and how its linguistic properties are encoded in these embeddings hasn't been perfectly straightforward. Therefore, rather than casting a wide net over all types of sentences, focusing on questions allows us to probe vector representations of natural language sentences that share similar linguistic properties, in this case the context of interrogative intent. Interrogative intent is a pragmatic phenomenon which makes us understand the semantic content of questions as raising issues or eliciting information. This in contrast with the traditional way of considering questions as bearers of classical propositions which differ from indicative sentences in pragmatic effects alone. From this angle, what interest us about questions is the manner in which they raise an issue, and how this correlates with the features of propositional content.\n",
    "\n",
    "More robust multilingual language model architectures are often placed in the spotlight due to the capacity of their vector embeddings to capture diverse semantic nuances in many languages. The main idea behind STS is to interpret similarity measures between vector representations of natural language sentences in a way that accounts for the degree to which the meanings and sometimes functions of these sentences overlap. Keeping this aim in mind, this report describes a new approach to STS which introduces a linguistic similarity measure that reflects typological variety in question formation patterns and overlap in interrogative intent. The main aim of this approach is to explore how multilingual LLMs represent the semantic content of questions. This area of research could benefit from more attention because understanding the relationship between sentences that express questions and internal vector representations of these sentences could help us better understand the nature of interrogative linguistic intent.\n",
    "\n",
    "Consider this pair of questions:\n",
    "\n",
    "1. A. Can I take my dog there? B. What is their pet policy?\n",
    "\n",
    "Although these questions evoke similar interrogative intent and raise similar issues, their structural differences demonstrate that sufficiently understanding a question's meaning requires comprehending both the issue it raises (often associated with its propositional content) and how it raises that issue (related to its interrogative type). Based on these two requirements, we adopt a working assumption that morphosyntactic markers provide information about the kind of proposition at issue, while typological features indicate the type of answer that would resolve the issue. Keeping this assumption we set off to investigate the following research questions:\n",
    "\n",
    "  *   Are questions that share a meaning and raise the same issue always represented similarly in multilingual language models?\n",
    "\n",
    "  *   Do all languages converge into a single way of representing linguistic features of questions in these models?\n",
    "\n",
    "We hypothesize that while questions with similar meanings and interrogative intent may exhibit similar vector representations, the specific linguistic features and typological differences across languages will lead to some divergence in how these questions are encoded in multilingual language models. By exploring these research questions, we seek to gain insights into the nature of interrogative linguistic intent and its representation in multilingual NLP models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea5e341",
   "metadata": {},
   "source": [
    "### Task Description: Multilingual Question Semantics\n",
    "\n",
    "The primary focus of this study is to investigate whether multilingual language models represent semantically similar questions with similar internal representations. While the choice of model depends on the task at hand, most state-of-the-art LLMs relevant for STS tasks are essentially variations of the Bi-encoder architectures described in (Reimers, Gurevych 2019). For this report we have chosen the paraphrase-multilingual-MiniLM-L12-v2 model, which is provided by the Sentence Transformers library. \n",
    "\n",
    "This report builds upon existing approaches to sentence similarity, and uses the cosine similarity measure to understand how much the model represents questions with similar semantics as having similar internal representations. This report breaks from traditional approaches in that it does not evaluate the performance of language models when predicting sentence similarity, nor does it attempt to measure similarity between sentences or documents. Instead, it treats cosine similarity as an analogue of linguistic similarity, and investigates whether similar meanings in natural language determine the similarity between internal model representations, and whether some linguistic features of natural language sentences have a greater effect on how meaning is represented in LLMs. More specifically, this report tests the following hypotheses:\n",
    "  *   *Morphosyntactic Hypothesis:* The co-occurrence of specific morphosyntactic markers in a sentence relates to its interrogative type. For example, we hypothesize that questions that include modal verbs are more often polar questions, quantification often co-occurs with wh-questions, and cleft questions are rarely polar questions.\n",
    "   *  *Typological Hypothesis:* Language specific word-order properties can affect the presence of some feature type combinations and correlate with the model's similarity scores. For example, in languages with obligatory question particles (e.g., Japanese), the presence of these particles may strongly correlate with the representation of interrogative intent in the model.\n",
    "\n",
    "To investigate these hypotheses, we adopt a standard approach with these three steps:\n",
    " 1. Generate embeddings: Take a question as input and generate a sentence embedding that represents its semantic meaning in a vector space, using the 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2' model.\n",
    "  2. Compare embeddings: Calculate the cosine similarity between the target and source embeddings, where a higher score indicates greater semantic similarity between the questions.\n",
    "3. Analyze Feature Co-occurrence: Relate the cosine similarity scores with the presence of different linguistic features of question sentences.\n",
    " The model description is essentially a sentence transformer that uses a Siamese network structure with a MiniLM-L12 transformer, as described by (Reimers, Gurevych 2019). Siamese network structures are an approach to sentence transformer architectures that aim to overcome the issue of generating high-quality, uniform-shape embeddings for variable-length sentences. (Reimers, Gurevych 2019) proposed an approach that uses a Siamese neural network architecture with a pre-trained BERT model to encode two sentences and compute their similarity based on the cosine distance between their embeddings. This approach is intended to address the limitations of traditional methods that rely on simple averaging or concatenation of word embeddings to represent sentences, which often fail to capture the complexity and nuance of natural language.   \n",
    "The analysis in this report is different from standard sentence scoring tasks, as the goal is not to evaluate the model's performance in predicting sentence similarity, but rather to probe the internal representations and how different features influence these representations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b88dee",
   "metadata": {
    "id": "WevMyyK2kmRG"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The multilingual question pair dataset was developed to support research on the representation of semantically similar questions in pretrained language models across diverse languages. The primary goal was to create a resource that enables investigation of how linguistic features and typological differences influence the encoding of interrogative intent and content.\n",
    "\n",
    "The dataset currently includes question pairs in five languages: Afrikaans (af), Arabic (ar), English (en), Indonesian (id), and Marathi (mr). These languages were selected to represent a range of language families and typological characteristics, while also considering the availability of parallel question-answer data from the NLLB corpus [citation needed]. The final dataset consists of ~100k question pairs, with ~[98k] unique questions across the five languages. Indonesian remains the most prevalent language, followed by Afrikaans, Marathi, Arabic and English. The data consists of translation pairs in (ar-en, mr-en, id-en, af-en). The english sentences of this dataset have been annotated with linguistic features from 2 categories. The first are morphosyntactic markers in the propositions of the question sentences, namely cleft, negation, modality, quantification, and comparison. The second category contains only question type features, namely either polar question, alternative question, wh-question, conditional question. Notably, each sentence can have several features from the first category, and a single type. Our research makes use of these features for investigating the interpretability of llm embeddings of these sentence pairs. First, the features in each sentence are encoded using a multi label binarizer to give multi label encodings, which are then passed on to a language model (sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) and analysis is ultimately conducted on the similarity between source and target(en) embeddings.\n",
    "\n",
    "Each example includes the source question, its English translation, language identifier, and the annotated linguistic features for the information/proposition and the question type. The dataset is provided in a structured CSV format, along with the original JSON files and the preprocessing scripts for transparency and reproducibility. For more details about dataset creation, and to make custom size test sets, see Appendix A.\n",
    "\n",
    "The dataset can be found at [link]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8633d765",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "The methodology used in this report is based on standard sentence embeddings analysis using cosine similarity. However, the objective of this analysis is not to evaluate the model's performance in downstream tasks, but to investigate how the model represents interrogative intent in the embeddings space, and the relation between linguistic features and the internal vector representations. \n",
    "\n",
    "To this end, the pipeline used includes the following steps:\n",
    "\n",
    "#### Embeddings Generation:\n",
    "   1.  **Feature Encoding**: Linguistic features in each question are encoded using a multi-label binarizer, resulting in multi-label encodings for each sentence.\n",
    "   2. **Sentence Encoding**: Both source (non-english) and target (english) questions are passed to the 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2' model, which produces a sentence embedding that represents its semantic meaning in a vector space. \n",
    "\n",
    "#### Similarity Measures:\n",
    " 1.  **Cosine Similarity**: The similarity between the source and target embeddings is calculated using cosine similarity. This metric is widely used for comparing sentence embeddings, as it measures the cosine of the angle between the two vectors. Cosine similarity is used in this report as an analogue of linguistic similarity between questions, so that we can investigate whether similar meanings in natural language determine the similarity between internal model representations, and whether some linguistic features of natural language sentences have a greater effect on how meaning is represented in LLMs.\n",
    "\n",
    "#### Qualitative and Quantitative Analysis:\n",
    "   1. **Language-Specific Analysis**: We investigate how mean cosine similarity scores and feature co-occurrences vary across the different languages. \n",
    "  2. **Feature-Specific Analysis:** We evaluate how different morphosyntactic features influence the similarity scores, and explore the specific interaction of those features when questions are represented as embeddings. This is done by systematically analyzing feature co-occurrences across different question types (polar, wh, alternative, conditional). We test specific hypotheses about how features relate to question types. We measure the co-occurrences of the selected features, and use the mean scores for cosine similarity to understand if these relations impact the internal representations of the model.\n",
    "  3. **Statistical tests**: We apply statistical tests such as t-tests to compare means, and interpret the p-values in relation to the hypotheses tested in each experimental section.\n",
    "  4.   **Visualization**: We use heatmaps and histograms to visually assess the data, and to identify patterns and relations between features, languages, and similarity scores. In particular, we generate a heatmap of co-occurences across different languages, to explore patterns in their relation.\n",
    "\n",
    "By adopting this approach, this report attempts to provide a comprehensive analysis that integrates both linguistic features and numerical evaluation of the relation between source and target sentence representations. This helps to get a better understanding of how multilingual language models encode meaning and interrogative intent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42f9b122",
   "metadata": {
    "id": "4hUYXHFkkmRJ"
   },
   "outputs": [],
   "source": [
    "# import the required libraries\n",
    "\n",
    "from sentence_transformers import SentenceTransformer # loads the model and generates embeddings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm as tqdm # optional, but useful for loading bar widgets\n",
    "from scipy import stats\n",
    "from sklearn.metrics.pairwise import cosine_similarity # For efficient cosine similarity calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "879dc8dc",
   "metadata": {
    "id": "pi_s9XdJkmRK"
   },
   "outputs": [],
   "source": [
    "# 1. Load the annotated and encoded data that has been processed and re-sized\n",
    "def load_data(filepath='nllb_qpairs_all_encoded.csv'):\n",
    "    # Loads the original csv data with the results of the MLE encoder, see appendix A.\n",
    "    data = pd.read_csv(filepath)\n",
    "    print(f'Data loaded, shape: {data.shape}')\n",
    "    return data\n",
    "\n",
    "data = load_data()\n",
    "\n",
    "# 2. Load model from sentence transformers, HF package allows us to do this without too much effort\n",
    "def load_model(model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'):\n",
    "    try:\n",
    "        model = SentenceTransformer(model_name)\n",
    "        print(f'Model {model_name} loaded successfully.')\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None\n",
    "\n",
    "model = load_model()\n",
    "\n",
    "# 3. Utility function to add word order to the dataframe\n",
    "def add_typological_features(data):\n",
    "    typological_features = {\n",
    "        'mr': {'word_order': 'SOV'},\n",
    "        'id': {'word_order': 'SVO'},\n",
    "        'ar': {'word_order': 'VSO'},\n",
    "        'af': {'word_order': 'V2'}\n",
    "    }\n",
    "    for lang, props in typological_features.items():\n",
    "        data[f'word_order_{lang}'] = data['language'].apply(lambda x: 1 if x == lang else 0) # add each language as a new col\n",
    "    data['word_order'] = data['language'].map(lambda x: typological_features[x]['word_order'])\n",
    "    print(\"Word Order features added to the dataframe\")\n",
    "    return data\n",
    "\n",
    "data = add_typological_features(data)\n",
    "\n",
    "# 4. Utility function to calculate feature count\n",
    "def add_feature_count(data):\n",
    "    feature_cols = [col for col in data.columns if col.startswith('f1_')]\n",
    "    data['feature_count'] = data[feature_cols].sum(axis=1)\n",
    "    print(\"Feature counts added to the dataframe\")\n",
    "    return data\n",
    "\n",
    "data = add_feature_count(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "674f0a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if saved locally, load the embeddings in a new variable and set embedding collection arrays\n",
    "\n",
    "def load_embeddings(source_path='source_embeddings_miniLM.npy', target_path='target_embeddings_miniLM.npy'):\n",
    "  try:\n",
    "    source_embeddings = np.load(source_path)\n",
    "    target_embeddings = np.load(target_path)\n",
    "    print(f'Embeddings loaded from {source_path} and {target_path}')\n",
    "  except Exception as e:\n",
    "    print(f'Error loading embeddings: {e}. Make sure the paths are correct')\n",
    "    return None, None\n",
    "\n",
    "  s_emb = np.array(source_embeddings, dtype = float)  # convert the embeddings into npy array to be used in cosine calculations\n",
    "  t_emb = np.array(target_embeddings, dtype = float)\n",
    "  return s_emb, t_emb\n",
    "\n",
    "s_emb, t_emb = load_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "83541516",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aaLcqXLVkmRN",
    "outputId": "ba0ba3e2-e67f-481a-ed78-bb8409e6e441"
   },
   "outputs": [],
   "source": [
    "# Calculate cosine similarity of all examples\n",
    "def calculate_similarities(s_emb, t_emb, data):\n",
    "  if s_emb is None or t_emb is None:\n",
    "     print('Embeddings not loaded. Aborting similarity calculations.')\n",
    "     return None\n",
    "  similarities = cosine_similarity(s_emb, t_emb)\n",
    "  #adds the list of similarity scores to the original data with annotations and encodings\n",
    "  data['cosine_similarity'] = np.diagonal(similarities)\n",
    "  print('Results of similarity analysis:')\n",
    "  print(data['cosine_similarity'].describe())\n",
    "  return data\n",
    "\n",
    "if s_emb is not None and t_emb is not None:\n",
    " data = calculate_similarities(s_emb, t_emb, data) # calculates cosine similarity and adds it to data\n",
    "    # Save the data with similarities (excluding the embeddings)\n",
    " if data is not None:\n",
    "    data.drop(columns=[col for col in data.columns if col.startswith('q') or col.startswith('source_q')], inplace=True)\n",
    "    data.to_csv('questions_with_similarities_miniLM.csv', index=False)\n",
    "    print(\"Saved data together with new similarities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6b5a06d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_language_similarity(data):\n",
    "  print(\"\\nLanguage-specific Similarity Stats:\")\n",
    "  for lang in data['language'].unique():\n",
    "    lang_data = data[data['language'] == lang]\n",
    "    lang_sims = lang_data['cosine_similarity']\n",
    "\n",
    "    print(f\"\\n{lang.upper()}:\")\n",
    "    print(f\"Mean similarity: {lang_sims.mean():.3f}\")\n",
    "    print(f\"Max similarity: {lang_sims.max():.3f}\")\n",
    "    print(f\"Min similarity: {lang_sims.min():.3f}\")\n",
    "\n",
    "    high_idx = lang_sims.idxmax()\n",
    "    print(f\"Highest similarity pair:\")\n",
    "    print(f\"Source: {lang_data.iloc[high_idx]['source']}\")\n",
    "    print(f\"Target: {lang_data.iloc[high_idx]['target']}\")\n",
    "\n",
    "    low_idx = lang_sims.idxmin()\n",
    "    print(f\"Lowest similarity pair:\")\n",
    "    print(f\"Source: {lang_data.iloc[low_idx]['source']}\")\n",
    "    print(f\"Target: {lang_data.iloc[low_idx]['target']}\")\n",
    "\n",
    "if data is not None:\n",
    "  analyze_language_similarity(data)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
